{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = 'C2T1_Train.csv'\n",
    " \n",
    "rawDF=pd.read_csv(filePath)\n",
    "\n",
    "rawDF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Column has almost all '?' values renderring it useless for the model to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.drop('weight', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop max_glu_serum & A1Cresult as it is mostly NuLL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.drop('max_glu_serum', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.drop('A1Cresult', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('C2T1_Train.csv')\n",
    "\n",
    "# Replace '?' with NaN\n",
    "df = df.replace('?', pd.NA)\n",
    "\n",
    "# Define columns again, ensuring we're working with the correct data types\n",
    "num_columns = [col for col in df.columns if df[col].dtype in ['int64', 'float64'] and col not in ['encounter_id2', 'patient_nbr2']]\n",
    "cat_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "\n",
    "# Impute missing values\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent', add_indicator=True)\n",
    "\n",
    "df[num_columns] = num_imputer.fit_transform(df[num_columns])\n",
    "# The categorical imputation needs to be handled differently to avoid the error.\n",
    "# Manually impute missing values for categorical columns\n",
    "for col in cat_columns:\n",
    "    # If a column is categorical, fill missing values with the mode (most frequent value)\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Proceed with OneHotEncoder for categorical variables\n",
    "onehot_encoder = OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore')\n",
    "encoded_features = onehot_encoder.fit_transform(df[cat_columns])\n",
    "encoded_features_df = pd.DataFrame(encoded_features, columns=onehot_encoder.get_feature_names_out(cat_columns))\n",
    "encoded_features_df.index = df.index  # Ensure alignment of indices\n",
    "\n",
    "# Remove original categorical columns and merge the encoded features\n",
    "df = df.drop(cat_columns, axis=1)\n",
    "df = pd.concat([df, encoded_features_df], axis=1)\n",
    "\n",
    "# Standardize numerical features (excluding identifiers)\n",
    "scaler = StandardScaler()\n",
    "df[num_columns] = scaler.fit_transform(df[num_columns])\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.to_csv('cleandata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C2T1_Train.csv')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "columns_to_process = [\n",
    "    \"metformin\", \"repaglinide\", \"nateglinide\",\n",
    "    \"chlorpropamide\", \"glimepiride\", \"acetohexamide\", \"glipizide\", \"glyburide\",\n",
    "    \"tolbutamide\", \"pioglitazone\", \"rosiglitazone\", \"acarbose\", \"miglitol\",\n",
    "    \"troglitazone\", \"tolazamide\", \"examide\", \"citoglipton\",\n",
    "    \"glyburide-metformin\", \"glipizide-metformin\", \"glimepiride-pioglitazone\",\n",
    "    \"metformin-rosiglitazone\", \"metformin-pioglitazone\", \"change\", 'gender'\n",
    "]\n",
    "\n",
    "\n",
    "race_columns = [\"race\"]\n",
    "\n",
    "# Define a function to apply the conversion\n",
    "def convert_value(value):\n",
    "    if value is None or pd.isnull(value):\n",
    "        return value\n",
    "    value_lower = value.lower()\n",
    "    return 0 if value_lower in ['none', 'no', 'female'] else 1\n",
    "\n",
    "# Apply the function to the specified columns\n",
    "for column in columns_to_process:\n",
    "    if column in df.columns:  # Check if the column exists in the DataFrame\n",
    "        df[column] = df[column].apply(convert_value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.drop('weight', axis=1, inplace=True)\n",
    "df.drop('max_glu_serum', axis=1, inplace=True)\n",
    "df.drop('A1Cresult', axis=1, inplace=True)\n",
    "\n",
    "df.to_csv('path_to_your_processed_csv_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "Caucasian          67515\n",
      "AfricanAmerican    17267\n",
      "?                   2207\n",
      "Hispanic            1834\n",
      "Other               1358\n",
      "Asian                585\n",
      "Name: count, dtype: int64\n",
      "race\n",
      "1.0    67515\n",
      "2.0    17267\n",
      "3.0     1834\n",
      "4.0     1358\n",
      "5.0      585\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('path_to_your_processed_csv_file.csv')\n",
    "race_value_counts = df['race'].value_counts()\n",
    "print(race_value_counts)\n",
    "\n",
    "\n",
    "\n",
    "race_mapping = {\n",
    "    'Caucasian': 1,\n",
    "    'AfricanAmerican': 2,\n",
    "    'Hispanic': 3,\n",
    "    'Other': 4,\n",
    "    'Asian': 5,\n",
    "    '?': np.nan  \n",
    "}\n",
    "\n",
    "\n",
    "df['race'] = df['race'].map(race_mapping)\n",
    "\n",
    "\n",
    "\n",
    "race_value_counts2= df['race'].value_counts()\n",
    "print(race_value_counts2)\n",
    "\n",
    "df.to_csv('path_to_your_processed_csv_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical_specialty\n",
      "?                                       43674\n",
      "InternalMedicine                        13657\n",
      "Family/GeneralPractice                   6906\n",
      "Emergency/Trauma                         6398\n",
      "Cardiology                               4939\n",
      "Surgery-General                          2778\n",
      "Nephrology                               1457\n",
      "Orthopedics                              1234\n",
      "Orthopedics-Reconstructive               1137\n",
      "Radiologist                               966\n",
      "Pulmonology                               814\n",
      "Psychiatry                                774\n",
      "Urology                                   627\n",
      "Surgery-Cardiovascular/Thoracic           623\n",
      "ObstetricsandGynecology                   619\n",
      "Gastroenterology                          484\n",
      "Surgery-Neuro                             446\n",
      "Surgery-Vascular                          416\n",
      "PhysicalMedicineandRehabilitation         383\n",
      "Oncology                                  314\n",
      "Pediatrics                                239\n",
      "Hematology/Oncology                       205\n",
      "Neurology                                 172\n",
      "Pediatrics-Endocrinology                  159\n",
      "Endocrinology                             120\n",
      "Otolaryngology                            117\n",
      "Surgery-Thoracic                          100\n",
      "Pediatrics-CriticalCare                    87\n",
      "Surgery-Cardiovascular                     85\n",
      "Psychology                                 83\n",
      "Podiatry                                   80\n",
      "Hematology                                 72\n",
      "Gynecology                                 54\n",
      "Radiology                                  53\n",
      "Hospitalist                                44\n",
      "Surgeon                                    44\n",
      "Surgery-Plastic                            34\n",
      "InfectiousDiseases                         34\n",
      "Osteopath                                  33\n",
      "SurgicalSpecialty                          32\n",
      "Ophthalmology                              31\n",
      "Pediatrics-Pulmonology                     25\n",
      "Obsterics&Gynecology-GynecologicOnco       25\n",
      "Anesthesiology-Pediatric                   19\n",
      "Pathology                                  17\n",
      "Obstetrics                                 14\n",
      "Rheumatology                               14\n",
      "Anesthesiology                             12\n",
      "Surgery-Colon&Rectal                       11\n",
      "Surgery-Maxillofacial                      10\n",
      "OutreachServices                           10\n",
      "Pediatrics-Neurology                       10\n",
      "PhysicianNotFound                           9\n",
      "Surgery-Pediatric                           8\n",
      "Endocrinology-Metabolism                    8\n",
      "AllergyandImmunology                        7\n",
      "Psychiatry-Child/Adolescent                 7\n",
      "Cardiology-Pediatric                        6\n",
      "DCPTEAM                                     6\n",
      "Dentistry                                   4\n",
      "Pediatrics-Hematology-Oncology              4\n",
      "Pediatrics-AllergyandImmunology             3\n",
      "Pediatrics-EmergencyMedicine                3\n",
      "Resident                                    2\n",
      "Proctology                                  1\n",
      "Psychiatry-Addictive                        1\n",
      "Dermatology                                 1\n",
      "SportsMedicine                              1\n",
      "Speech                                      1\n",
      "Pediatrics-InfectiousDiseases               1\n",
      "Neurophysiology                             1\n",
      "Surgery-PlasticwithinHeadandNeck            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('path_to_your_processed_csv_file.csv')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "medical_specialty_value_counts = df['medical_specialty'].value_counts()\n",
    "\n",
    "\n",
    "print(medical_specialty_value_counts)\n",
    "medical_specialty_value_counts.to_csv('medical_specialty_value_counts.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('path_to_your_processed_csv_file.csv')\n",
    "\n",
    "df.drop('medical_specialty', axis=1, inplace=True)\n",
    "\n",
    "df.to_csv('path_to_your_processed_csv_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id2  patient_nbr2  race  gender  age  admission_type_id   \n",
      "0           5283      48330653   1.0       0   85                  2  \\\n",
      "1           8499      63555809   1.0       0   95                  3   \n",
      "2           9441      42519137   1.0       1   45                  1   \n",
      "3          20997      89868902   2.0       0   45                  1   \n",
      "4          28515      82637321   1.0       1   55                  2   \n",
      "\n",
      "   discharge_disposition_id  admission_source_id  time_in_hospital payer_code   \n",
      "0                         1                    4                13          ?  \\\n",
      "1                         3                    4                12          ?   \n",
      "2                         1                    7                 1          ?   \n",
      "3                         1                    7                 9          ?   \n",
      "4                         1                    2                 3          ?   \n",
      "\n",
      "   ...  citoglipton  insulin  glyburide-metformin  glipizide-metformin   \n",
      "0  ...            0   Steady                    0                    0  \\\n",
      "1  ...            0   Steady                    0                    0   \n",
      "2  ...            0   Steady                    0                    0   \n",
      "3  ...            0   Steady                    0                    0   \n",
      "4  ...            0   Steady                    0                    0   \n",
      "\n",
      "   glimepiride-pioglitazone  metformin-rosiglitazone metformin-pioglitazone   \n",
      "0                         0                        0                      0  \\\n",
      "1                         0                        0                      0   \n",
      "2                         0                        0                      0   \n",
      "3                         0                        0                      0   \n",
      "4                         0                        0                      0   \n",
      "\n",
      "  change diabetesMed  readmitted  \n",
      "0      1         Yes          NO  \n",
      "1      1         Yes          NO  \n",
      "2      1         Yes          NO  \n",
      "3      0         Yes         >30  \n",
      "4      0         Yes         >30  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "def convert_age(age_range):\n",
    "    lower, upper = age_range[1:-1].split('-')\n",
    "    midpoint = (int(lower) + int(upper)) // 2\n",
    "    return midpoint\n",
    "\n",
    "\n",
    "df = pd.read_csv('path_to_your_processed_csv_file.csv')\n",
    "\n",
    "# Apply the conversion function to the 'age' column\n",
    "df['age'] = df['age'].apply(convert_age)\n",
    "\n",
    "# Display the first few rows to ensure the conversion worked as expected\n",
    "print(df.head())\n",
    "\n",
    "# Save the processed DataFrame back to a new CSV file\n",
    "df.to_csv('path_to_your_processed_csv_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('path_to_your_processed_csv_file.csv')\n",
    "scaler = StandardScaler()\n",
    "num_columns = [col for col in df.columns if df[col].dtype in ['int64', 'float64'] and col not in ['encounter_id2', 'patient_nbr2']]\n",
    "df[num_columns] = scaler.fit_transform(df[num_columns])\n",
    "df.to_csv('path_to_your_processed_csv_file1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readmitted\n",
      "0    49361\n",
      "1    41405\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('path_to_your_processed_csv_file1.csv')\n",
    "\n",
    "\n",
    "# Convert 'readmitted' values: 0 for 'NO', 1 for any other value\n",
    "df['readmitted'] = df['readmitted'].apply(lambda x: 0 if x == 'NO' else 1)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV if needed\n",
    "df.to_csv('path_to_your_processed_csv_file1.csv', index=False)\n",
    "\n",
    "# Verify the changes\n",
    "print(df['readmitted'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ym/0_vtz1kx4_5fl0q9222rj5n40000gn/T/ipykernel_36926/3717014807.py:1: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('path_to_your_processed_csv_file1.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('path_to_your_processed_csv_file1.csv')\n",
    "\n",
    "df.drop('payer_code', axis=1, inplace=True)\n",
    "\n",
    "df.to_csv('path_to_your_processed_csv_file1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encounter_id2  patient_nbr2      race    gender       age   \n",
      "0           5283      48330653 -0.473075 -0.928409  1.195421  \\\n",
      "1           8499      63555809 -0.473075 -0.928409  1.820909   \n",
      "2           9441      42519137 -0.473075  1.077112 -1.306535   \n",
      "3          20997      89868902  1.058792 -0.928409 -1.306535   \n",
      "4          28515      82637321 -0.473075  1.077112 -0.681046   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id   \n",
      "0          -0.017468                 -0.517817            -0.445388  \\\n",
      "1           0.670080                 -0.144628            -0.445388   \n",
      "2          -0.705015                 -0.517817             0.277931   \n",
      "3          -0.705015                 -0.517817             0.277931   \n",
      "4          -0.017468                 -0.517817            -0.927601   \n",
      "\n",
      "   time_in_hospital payer_code  ...  citoglipton  insulin   \n",
      "0          2.867647          ?  ...          0.0   Steady  \\\n",
      "1          2.533853          ?  ...          0.0   Steady   \n",
      "2         -1.137886          ?  ...          0.0   Steady   \n",
      "3          1.532469          ?  ...          0.0   Steady   \n",
      "4         -0.470297          ?  ...          0.0   Steady   \n",
      "\n",
      "   glyburide-metformin  glipizide-metformin  glimepiride-pioglitazone   \n",
      "0            -0.079285            -0.009389                 -0.003319  \\\n",
      "1            -0.079285            -0.009389                 -0.003319   \n",
      "2            -0.079285            -0.009389                 -0.003319   \n",
      "3            -0.079285            -0.009389                 -0.003319   \n",
      "4            -0.079285            -0.009389                 -0.003319   \n",
      "\n",
      "   metformin-rosiglitazone metformin-pioglitazone    change diabetesMed   \n",
      "0                -0.004694              -0.003319  1.099012         Yes  \\\n",
      "1                -0.004694              -0.003319  1.099012         Yes   \n",
      "2                -0.004694              -0.003319  1.099012         Yes   \n",
      "3                -0.004694              -0.003319 -0.909908         Yes   \n",
      "4                -0.004694              -0.003319 -0.909908         Yes   \n",
      "\n",
      "   readmitted  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           1  \n",
      "4           1  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('path_to_your_processed_csv_file1.csv')\n",
    "\n",
    "# Replace all NaN values with 0s\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Verify the changes by displaying the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.to_csv('path_to_your_processed_csv_file1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('path_to_your_processed_csv_file1.csv')\n",
    "# Replace all NaN values with 0s\n",
    "df.replace('?', 0, inplace=True)\n",
    "\n",
    "df.to_csv('path_to_your_processed_csv_file1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insulin\n",
      "0    42793\n",
      "1    27876\n",
      "3    10487\n",
      "2     9610\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('path_to_your_processed_csv_file1.csv')\n",
    "\n",
    "\n",
    "insulin_mapping = {\n",
    "    'No': 0,\n",
    "    'Steady': 1,\n",
    "    'Up': 2,\n",
    "    'Down': 3\n",
    "}\n",
    "\n",
    "\n",
    "df['insulin'] = df['insulin'].map(insulin_mapping)\n",
    "\n",
    "\n",
    "print(df['insulin'].value_counts())\n",
    "\n",
    "df.to_csv('path_to_your_processed_csv_file1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetesMed\n",
      "1    69573\n",
      "0    21193\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('path_to_your_processed_csv_file1.csv')\n",
    "\n",
    "# Define the mapping for the 'insulin' column\n",
    "insulin_mapping = {\n",
    "    'No': 0,\n",
    "    'Yes': 1,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "df['diabetesMed'] = df['diabetesMed'].map(insulin_mapping)\n",
    "\n",
    "\n",
    "print(df['diabetesMed'].value_counts())\n",
    "\n",
    "df.to_csv('path_to_your_processed_csv_file1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Random Forest Model\u001b[39;00m\n\u001b[1;32m     47\u001b[0m rf_model \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, class_weight\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m rf_model\u001b[39m.\u001b[39;49mfit(X_train, y_train)  \u001b[39m# Training with NumPy arrays\u001b[39;00m\n\u001b[1;32m     49\u001b[0m rf_y_pred \u001b[39m=\u001b[39m rf_model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     51\u001b[0m \u001b[39m# Evaluate Random Forest Model\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    477\u001b[0m )(\n\u001b[1;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    479\u001b[0m         t,\n\u001b[1;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    481\u001b[0m         X,\n\u001b[1;32m    482\u001b[0m         y,\n\u001b[1;32m    483\u001b[0m         sample_weight,\n\u001b[1;32m    484\u001b[0m         i,\n\u001b[1;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    489\u001b[0m     )\n\u001b[1;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df = pd.read_csv('path_to_your_processed_csv_file1.csv')\n",
    "\n",
    "\n",
    "features = [\n",
    "    \"gender\", \"age\", \"admission_type_id\", \"discharge_disposition_id\",\n",
    "    \"admission_source_id\", \"time_in_hospital\", \"num_lab_procedures\",\n",
    "    \"num_procedures\", \"num_medications\", \"number_outpatient\", \"number_emergency\",\n",
    "    \"number_inpatient\", \"number_diagnoses\",\n",
    "    \"metformin\", \"repaglinide\", \"nateglinide\", \"chlorpropamide\", \"glimepiride\",\n",
    "    \"acetohexamide\", \"glipizide\", \"glyburide\", \"tolbutamide\", \"pioglitazone\",\n",
    "    \"rosiglitazone\", \"acarbose\", \"miglitol\", \"troglitazone\", \"tolazamide\",\n",
    "    \"examide\", \"citoglipton\", \"insulin\", \"glyburide-metformin\", \"glipizide-metformin\",\n",
    "    \"glimepiride-pioglitazone\", \"metformin-rosiglitazone\", \"metformin-pioglitazone\",\n",
    "    \"change\", \"diabetesMed\"\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['readmitted']  # Target variable\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X, dtype=np.float32)\n",
    "y = np.array(y, dtype=np.float32)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)  # Training with NumPy arrays\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate Random Forest Model\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_f1 = f1_score(y_test, rf_y_pred)\n",
    "rf_conf_matrix = confusion_matrix(y_test, rf_y_pred)\n",
    "\n",
    "joblib.dump(rf_model, 'randomforest.pkl')\n",
    "\n",
    "print(\"Random Forest Model Evaluation:\")\n",
    "print(f\"Accuracy: {rf_accuracy * 100:.2f}%\")\n",
    "print(f\"F1 Score: {rf_f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(rf_conf_matrix)\n",
    "\n",
    "class EnhancedNNModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(EnhancedNNModel, self).__init__()\n",
    "        # Increasing the depth and complexity of the network\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "dl_model = EnhancedNNModel(input_dim)\n",
    "\n",
    "# Define loss function and optimizer for deep learning model\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(dl_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop for deep learning model\n",
    "num_epochs = 500\n",
    "dl_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = dl_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor.view(-1, 1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    dl_losses.append(loss.item())\n",
    "\n",
    "# Predictions for deep learning model\n",
    "with torch.no_grad():\n",
    "    dl_y_pred = dl_model(X_test_tensor)\n",
    "    dl_y_pred = (dl_y_pred >= 0.5).float()\n",
    "\n",
    "# Convert tensors back to NumPy arrays for evaluation\n",
    "dl_y_pred = dl_y_pred.numpy()\n",
    "dl_y_test = y_test_tensor.numpy()\n",
    "\n",
    "# Evaluate Deep Learning Model\n",
    "dl_accuracy = accuracy_score(dl_y_test, dl_y_pred)\n",
    "dl_f1 = f1_score(dl_y_test, dl_y_pred)\n",
    "dl_conf_matrix = confusion_matrix(dl_y_test, dl_y_pred)\n",
    "\n",
    "print(\"\\nDeep Learning Model Evaluation:\")\n",
    "print(f\"Accuracy: {dl_accuracy * 100:.2f}%\")\n",
    "print(f\"F1 Score: {dl_f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(dl_conf_matrix)\n",
    "\n",
    "# Visualize the loss curve for deep learning model\n",
    "plt.plot(dl_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss for Deep Learning Model\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# AdaBoost Model\n",
    "ab_model = AdaBoostClassifier(n_estimators=500, random_state=42)\n",
    "ab_model.fit(X_train, y_train)  # Training\n",
    "ab_y_pred = ab_model.predict(X_test)\n",
    "\n",
    "# Evaluate AdaBoost Model\n",
    "ab_accuracy = accuracy_score(y_test, ab_y_pred)\n",
    "ab_f1 = f1_score(y_test, ab_y_pred)\n",
    "ab_conf_matrix = confusion_matrix(y_test, ab_y_pred)\n",
    "\n",
    "print(\"AdaBoost Model Evaluation:\")\n",
    "print(f\"Accuracy: {ab_accuracy * 100:.2f}%\")\n",
    "print(f\"F1 Score: {ab_f1:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(ab_conf_matrix)\n",
    "# Save the models\n",
    "#joblib.dump(rf_model, '/Users/amirrezarafati/Downloads/CapsotneModel/RB/Repo/Capstone/Amir/Data/RandomForestModel.pkl')\n",
    "#torch.save(dl_model.state_dict(), '/Users/amirrezarafati/Downloads/CapsotneModel/RB/Repo/Capstone/Amir/Data/DeepLearningModel.pth')\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Support Vector Machine (SVM) Model\n",
    "svm_model = SVC(class_weight='balanced')\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate SVM Model\n",
    "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "svm_f1 = f1_score(y_test, svm_y_pred)\n",
    "svm_conf_matrix = confusion_matrix(y_test, svm_y_pred)\n",
    "\n",
    "print(\"\\nSVM Model Evaluation:\")\n",
    "print(f\"Accuracy: {svm_accuracy * 100:.2f}%\")\n",
    "print(f\"F1 Score: \", svm_f1)\n",
    "print(\"Confusion Matrix:\", svm_conf_matrix)\n",
    "\n",
    "\n",
    "joblib.dump(svm_model, '/Users/amirrezarafati/Downloads/CapsotneModel/RB/Repo/Capstone/Amir/Data/PredictionModelSVM.pkl')\n",
    "\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, svm_y_pred, classes=[0, 1], model_name='SVM')\n",
    "\n",
    "plot_confusion_matrix(y_test, rf_y_pred, classes=[0, 1], model_name='Random Forest')\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_test, ab_y_pred, classes=[0, 1], model_name='AdaBoost')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetesMed\n",
      "1    8790\n",
      "0    2210\n",
      "Name: count, dtype: int64\n",
      "insulin\n",
      "0    4590\n",
      "1    2973\n",
      "3    1731\n",
      "2    1706\n",
      "Name: count, dtype: int64\n",
      "   encounter_id  patient_nbr             race  gender      age weight   \n",
      "0     168899775     88565423        Caucasian    Male  [60-70)      0  \\\n",
      "1     168901359     88590695        Caucasian    Male  [60-70)      0   \n",
      "2     168903045     61086362  AfricanAmerican  Female  [70-80)      0   \n",
      "3     168903927     85993970        Caucasian    Male  [70-80)      0   \n",
      "4     168904515     45884291        Caucasian    Male  [50-60)      0   \n",
      "\n",
      "   admission_type_id  discharge_disposition_id  admission_source_id   \n",
      "0                  3                         1                    1  \\\n",
      "1                  5                         1                    1   \n",
      "2                  1                         1                    1   \n",
      "3                  1                         1                    7   \n",
      "4                  2                         1                    2   \n",
      "\n",
      "   time_in_hospital  ... citoglipton insulin  glyburide-metformin   \n",
      "0                 2  ...          No       0                   No  \\\n",
      "1                 1  ...          No       0                   No   \n",
      "2                 2  ...          No       0                   No   \n",
      "3                 2  ...          No       1                   No   \n",
      "4                 5  ...          No       3                   No   \n",
      "\n",
      "   glipizide-metformin  glimepiride-pioglitazone  metformin-rosiglitazone   \n",
      "0                   No                        No                       No  \\\n",
      "1                   No                        No                       No   \n",
      "2                   No                        No                       No   \n",
      "3                   No                        No                       No   \n",
      "4                   No                        No                       No   \n",
      "\n",
      "   metformin-pioglitazone  change diabetesMed readmitted  \n",
      "0                      No      Ch           1        0.0  \n",
      "1                      No      No           1        0.0  \n",
      "2                      No      No           0        0.0  \n",
      "3                      No      No           1        0.0  \n",
      "4                      No      Ch           1        0.0  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "df = pd.read_csv('C2T1_Test.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "insulin_mapping = {\n",
    "    'No': 0,\n",
    "    'Yes': 1,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "df['diabetesMed'] = df['diabetesMed'].map(insulin_mapping)\n",
    "\n",
    "\n",
    "print(df['diabetesMed'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "insulin_mapping = {\n",
    "    'No': 0,\n",
    "    'Steady': 1,\n",
    "    'Up': 2,\n",
    "    'Down': 3\n",
    "}\n",
    "\n",
    "\n",
    "df['insulin'] = df['insulin'].map(insulin_mapping)\n",
    "\n",
    "\n",
    "print(df['insulin'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.replace('?', 0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "df.drop('payer_code', axis=1, inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "num_columns = [col for col in df.columns if df[col].dtype in ['int64', 'float64'] and col not in ['encounter_id2', 'patient_nbr2']]\n",
    "df[num_columns] = scaler.fit_transform(df[num_columns])\n",
    "\n",
    "\n",
    "\n",
    "def convert_age(age_range):\n",
    "    lower, upper = age_range[1:-1].split('-')\n",
    "    midpoint = (int(lower) + int(upper)) // 2\n",
    "    return midpoint\n",
    "\n",
    "\n",
    "df['age'] = df['age'].apply(convert_age)\n",
    "\n",
    "\n",
    "df.drop('medical_specialty', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "race_mapping = {\n",
    "    'Caucasian': 1,\n",
    "    'AfricanAmerican': 2,\n",
    "    'Hispanic': 3,\n",
    "    'Other': 4,\n",
    "    'Asian': 5,\n",
    "    '?': np.nan  \n",
    "}\n",
    "\n",
    "\n",
    "df['race'] = df['race'].map(race_mapping)\n",
    "\n",
    "\n",
    "\n",
    "race_value_counts2= df['race'].value_counts()\n",
    "\n",
    "df.drop('weight', axis=1, inplace=True)\n",
    "df.drop('max_glu_serum', axis=1, inplace=True)\n",
    "df.drop('A1Cresult', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "columns_to_process = [\n",
    "    \"metformin\", \"repaglinide\", \"nateglinide\",\n",
    "    \"chlorpropamide\", \"glimepiride\", \"acetohexamide\", \"glipizide\", \"glyburide\",\n",
    "    \"tolbutamide\", \"pioglitazone\", \"rosiglitazone\", \"acarbose\", \"miglitol\",\n",
    "    \"troglitazone\", \"tolazamide\", \"examide\", \"citoglipton\",\n",
    "    \"glyburide-metformin\", \"glipizide-metformin\", \"glimepiride-pioglitazone\",\n",
    "    \"metformin-rosiglitazone\", \"metformin-pioglitazone\", \"change\", 'gender'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_value(value):\n",
    "    if value is None or pd.isnull(value):\n",
    "        return value\n",
    "    value_lower = value.lower()\n",
    "    return 0 if value_lower in ['none', 'no', 'female'] else 1\n",
    "\n",
    "\n",
    "for column in columns_to_process:\n",
    "    if column in df.columns:  \n",
    "        df[column] = df[column].apply(convert_value)\n",
    "\n",
    "\n",
    "df.to_csv('C2T1_Testclean.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encounter_id                 0\n",
      "patient_nbr                  0\n",
      "race                        66\n",
      "gender                       0\n",
      "age                          0\n",
      "admission_type_id            0\n",
      "discharge_disposition_id     0\n",
      "admission_source_id          0\n",
      "time_in_hospital             0\n",
      "num_lab_procedures           0\n",
      "num_procedures               0\n",
      "num_medications              0\n",
      "number_outpatient            0\n",
      "number_emergency             0\n",
      "number_inpatient             0\n",
      "diag_1                       0\n",
      "diag_2                       0\n",
      "diag_3                       0\n",
      "number_diagnoses             0\n",
      "metformin                    0\n",
      "repaglinide                  0\n",
      "nateglinide                  0\n",
      "chlorpropamide               0\n",
      "glimepiride                  0\n",
      "acetohexamide                0\n",
      "glipizide                    0\n",
      "glyburide                    0\n",
      "tolbutamide                  0\n",
      "pioglitazone                 0\n",
      "rosiglitazone                0\n",
      "acarbose                     0\n",
      "miglitol                     0\n",
      "troglitazone                 0\n",
      "tolazamide                   0\n",
      "examide                      0\n",
      "citoglipton                  0\n",
      "insulin                      0\n",
      "glyburide-metformin          0\n",
      "glipizide-metformin          0\n",
      "glimepiride-pioglitazone     0\n",
      "metformin-rosiglitazone      0\n",
      "metformin-pioglitazone       0\n",
      "change                       0\n",
      "diabetesMed                  0\n",
      "readmitted                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('C2T1_Testclean.csv')\n",
    "\n",
    "df_cleaned = df.dropna(subset=[\"race\"])\n",
    "\n",
    "\n",
    "nan_counts = df.isna().sum()\n",
    "\n",
    "df_cleaned.to_csv('C2T1_Testclean.csv', index=False)\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:413: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('C2T1_Testclean.csv')\n",
    "\n",
    "\n",
    "features = [\n",
    "     \"gender\", \"age\", \"admission_type_id\", \"discharge_disposition_id\",\n",
    "    \"admission_source_id\", \"time_in_hospital\", \"num_lab_procedures\",\n",
    "    \"num_procedures\", \"num_medications\", \"number_outpatient\", \"number_emergency\",\n",
    "    \"number_inpatient\", \"number_diagnoses\",\n",
    "    \"metformin\", \"repaglinide\", \"nateglinide\", \"chlorpropamide\", \"glimepiride\",\n",
    "    \"acetohexamide\", \"glipizide\", \"glyburide\", \"tolbutamide\", \"pioglitazone\",\n",
    "    \"rosiglitazone\", \"acarbose\", \"miglitol\", \"troglitazone\", \"tolazamide\",\n",
    "    \"examide\", \"citoglipton\", \"insulin\", \"glyburide-metformin\", \"glipizide-metformin\",\n",
    "    \"glimepiride-pioglitazone\", \"metformin-rosiglitazone\", \"metformin-pioglitazone\",\n",
    "    \"change\", \"diabetesMed\"\n",
    "]\n",
    "\n",
    "\n",
    "X = df[features]\n",
    "\n",
    "\n",
    "model_path = 'randomforest.pkl'\n",
    "rf_model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "predictions = rf_model.predict(X)\n",
    "\n",
    "\n",
    "df['predicted_readmission'] = predictions\n",
    "\n",
    "\n",
    "output_path = 'Predictions.csv'\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Predictions.csv')\n",
    "\n",
    "\n",
    "columns_to_remove = [\n",
    "    \"race\", \"gender\", \"age\", \"admission_type_id\", \"discharge_disposition_id\",\n",
    "    \"admission_source_id\", \"time_in_hospital\", \"num_lab_procedures\",\n",
    "    \"num_procedures\", \"num_medications\", \"number_outpatient\", \"number_emergency\",\n",
    "    \"number_inpatient\", \"diag_1\", \"diag_2\", \"diag_3\", \"number_diagnoses\",\n",
    "    \"metformin\", \"repaglinide\", \"nateglinide\", \"chlorpropamide\", \"glimepiride\",\n",
    "    \"acetohexamide\", \"glipizide\", \"glyburide\", \"tolbutamide\", \"pioglitazone\",\n",
    "    \"rosiglitazone\", \"acarbose\", \"miglitol\", \"troglitazone\", \"tolazamide\",\n",
    "    \"examide\", \"citoglipton\", \"insulin\", \"glyburide-metformin\", \"glipizide-metformin\",\n",
    "    \"glimepiride-pioglitazone\", \"metformin-rosiglitazone\", \"metformin-pioglitazone\",\n",
    "    \"change\", \"diabetesMed\"\n",
    "]\n",
    "\n",
    "\n",
    "df.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "\n",
    "df.to_csv('Predictions.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Readmitted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Readmitted'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m conversion_dict \u001b[39m=\u001b[39m {\u001b[39m1\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mYes\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mNo\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[1;32m      9\u001b[0m \u001b[39m# Apply the mapping to the specified column\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mReadmitted\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mReadmitted\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mmap(conversion_dict)\n\u001b[1;32m     11\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mPredicted_readmitted\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mPredicted_readmitted\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(conversion_dict)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Save the modified DataFrame back to a CSV file\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Readmitted'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('Predictions.csv')\n",
    "\n",
    "\n",
    "conversion_dict = {1: 'Yes', 0: 'No'}\n",
    "\n",
    "# Apply the mapping to the specified column\n",
    "df['readmitted'] = df['readmitted'].map(conversion_dict)\n",
    "df['predicted_readmission'] = df['predicted_readmission'].map(conversion_dict)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv('C2T1_Test_Labelled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
